{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF Text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parakhchaudhary/opt/anaconda3/envs/chaenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import geocoder\n",
    "import pymysql\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "from settings import *\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAEMATOLOGY\n",
      "COMPLETE BLOOD COUNT (CBC)\n",
      "TEST\n",
      "VALUE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text += page.get_text()\n",
    "    document.close()\n",
    "    return text.strip()\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"cbc-report-format.pdf\")\n",
    "print(pdf_text[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_specific_intent_restriction(generated_text):\n",
    "    message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in verifying whether provided content is within restricted boundary\"},\n",
    "            {\"role\": \"user\", \"content\": f\"A chat Agent has generated the following summary of a PDF file. Please verify if the summary is of a Medical Health report. Reply with Yes or No only.:\\n\\n{generated_text}\"}\n",
    "        ]\n",
    "    response = monster_client.chat.completions.create(\n",
    "        model=monster_ai_model_name[llm_name],\n",
    "        messages=message,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    binary_classifier = response.choices[0].message.content\n",
    "\n",
    "    if binary_classifier == \"Yes\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(user_input, file=None):\n",
    "    try:\n",
    "        # If a file is uploaded, extract text\n",
    "        pdf_text = \"\"\n",
    "        if file:\n",
    "            pdf_text = extract_text_from_pdf(file.name)\n",
    "            if not pdf_text.strip():\n",
    "                return \"The uploaded PDF appears to be empty. Please try with a valid medical report.\"\n",
    "\n",
    "        # Create context based on input and optional PDF content\n",
    "        context = f\"Medical report content: {pdf_text}\" if file else \"\"\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in answering questions about medical reports.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}\\n\\n{user_input}\"}\n",
    "        ]\n",
    "\n",
    "        # Generate response using Monster API\n",
    "        response = monster_client.chat.completions.create(\n",
    "            model=monster_ai_model_name[llm_name],\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        # Extract the chat response\n",
    "        generated_text = response.choices[0].message.content\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant code ##NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import models, QdrantClient\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Initialize the encoder and Qdrant client\n",
    "# encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# client = QdrantClient(\":memory:\")  # For in-memory use; replace with cluster host info for persistent storage\n",
    "\n",
    "\n",
    "# collection_name = \"medical_reports\"\n",
    "# client.create_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vectors_config=models.VectorParams(\n",
    "#         size=encoder.get_sentence_embedding_dimension(),  # Vector size is defined by the encoder\n",
    "#         distance=models.Distance.COSINE,  # Use cosine similarity\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# def chunk_text(text, chunk_size=500):\n",
    "#     return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# chunks = chunk_text(pdf_text)\n",
    "\n",
    "# client.upload_points(\n",
    "#     collection_name=collection_name,\n",
    "#     points=[\n",
    "#         models.PointStruct(\n",
    "#             id=idx,\n",
    "#             vector=encoder.encode(chunk).tolist(),\n",
    "#             payload={\"chunk\": chunk}\n",
    "#         )\n",
    "#         for idx, chunk in enumerate(chunks)\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(f\"Stored {len(chunks)} chunks in Qdrant.\")\n",
    "\n",
    "\n",
    "# def query_relevant_chunks(query_text, top_k=5):\n",
    "#     query_vector = encoder.encode(query_text).tolist()\n",
    "#     hits = client.query_points(\n",
    "#         collection_name=collection_name,\n",
    "#         query=query_vector,\n",
    "#         limit=top_k,\n",
    "#     ).points\n",
    "#     return [hit.payload[\"chunk\"] for hit in hits]\n",
    "\n",
    "# query = \"summarize the medical findings\"\n",
    "# relevant_chunks = query_relevant_chunks(query)\n",
    "\n",
    "# print(\"Relevant chunks for summarization:\")\n",
    "# for chunk in relevant_chunks:\n",
    "#     print(chunk[:200], \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# from settings import *\n",
    "# import os\n",
    "\n",
    "# generation_model_name: str\n",
    "# temperature: float = 0.9\n",
    "# top_p = 0.9\n",
    "# max_tokens: int = 2048\n",
    "# stream: bool = True\n",
    "# llm_name: str = \"Meta-Llama\"\n",
    "\n",
    "# monster_client = OpenAI(\n",
    "#     base_url=\"https://llm.monsterapi.ai/v1/\",\n",
    "#     api_key=str(MONSTER_API_KEY)\n",
    "# )\n",
    "\n",
    "# monster_ai_model_name = {\n",
    "#     \"Google-Gemma\": \"google/gemma-2-9b-it\",\n",
    "#     \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     \"Microsoft-Phi\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     \"Meta-Llama\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "# }\n",
    "\n",
    "# # Context and Summarization Prompt\n",
    "# message = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in summarizing complex medical reports.\"},\n",
    "#     {\"role\": \"user\", \"content\": f\"Summarize the following medical report in simple terms. Focus on the key findings, diagnoses, and recommendations. Report text:\\n\\n{pdf_text}\"}\n",
    "# ]\n",
    "\n",
    "# # Generate Summary\n",
    "# response = monster_client.chat.completions.create(\n",
    "#     model=monster_ai_model_name[llm_name],\n",
    "#     messages=message,\n",
    "#     temperature=temperature,\n",
    "#     top_p=top_p,\n",
    "#     max_tokens=max_tokens,\n",
    "#     stream=stream\n",
    "# )\n",
    "\n",
    "# # Collect Generated Text\n",
    "# generated_text = \"\"\n",
    "# for chunk in response:\n",
    "#     if chunk.choices[0].delta.content is not None:\n",
    "#         generated_text += chunk.choices[0].delta.content\n",
    "\n",
    "# print(\"Generated Summary:\")\n",
    "# print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.16.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PDF Text Extraction Function\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text += page.get_text()\n",
    "    document.close()\n",
    "    return text.strip()\n",
    "\n",
    "# Monster API Client Configuration\n",
    "generation_model_name: str\n",
    "temperature: float = 0.9\n",
    "top_p = 0.9\n",
    "max_tokens: int = 2048\n",
    "stream: bool = False  # Set to False for simplicity in Gradio\n",
    "llm_name: str = \"Meta-Llama\"\n",
    "\n",
    "monster_client = OpenAI(\n",
    "    base_url=\"https://llm.monsterapi.ai/v1/\",\n",
    "    api_key=str(MONSTER_API_KEY)\n",
    ")\n",
    "\n",
    "monster_ai_model_name = {\n",
    "    \"Google-Gemma\": \"google/gemma-2-9b-it\",\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"Microsoft-Phi\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"Meta-Llama\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "}\n",
    "\n",
    "# Summarization Function\n",
    "def summarize_pdf(file):\n",
    "    try:\n",
    "        # Validate file format\n",
    "        if not file.name.endswith(\".pdf\"):\n",
    "            return \"Invalid file format. Please upload a PDF.\"\n",
    "\n",
    "        # Extract text from the uploaded PDF\n",
    "        pdf_text = extract_text_from_pdf(file.name)\n",
    "        if not pdf_text.strip():\n",
    "            return \"The uploaded PDF appears to be empty. Please try with a valid medical report.\"\n",
    "\n",
    "        # Create context for the AI summarization\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in summarizing complex medical reports. Strict instruction: Only summarise a report if it is a medical report. Otherwise, reply with 'Attached PDF is not a medical report'\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following medical report in simple terms. Focus on the key findings, diagnoses, and recommendations. Report text:\\n\\n{pdf_text}\"}\n",
    "        ]\n",
    "\n",
    "        # Generate summary using Monster API\n",
    "        response = monster_client.chat.completions.create(\n",
    "            model=monster_ai_model_name[llm_name],\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        # Extract the summary from the response\n",
    "        generated_text = response.choices[0].message.content\n",
    "\n",
    "        # validation \n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "interface = gr.Interface(\n",
    "    fn=chat_with_llm,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, placeholder=\"Type your question here...\", label=\"Ask a Question\"),\n",
    "        gr.File(label=\"Optional: Upload your Medical Report (PDF)\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"AI's Response\"),\n",
    "    title=\"Medical Report Chat Assistant\",\n",
    "    description=(\n",
    "        \"Ask questions about your medical report or any medical context. \"\n",
    "        \"Optionally, upload a PDF report for more tailored answers.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the Interface\n",
    "interface.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
